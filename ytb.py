import sys #library buat terima inputan dari php
import json

"""IIR A KHUSUS KITA SEMUA AMIN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1XvlkYOUplj1PkaUULsy-yuDMfJOVJq

#**Using Youtube API**

##Steps:
```
1. Buat akun google terus create api service, buat tutorial detailnya aku ngikut di youtube api :)
2. Buat suatu project namanya bebas
3. Pilih youtube api v3, install api tersebut
4. Setelah install API, itu nanti kita klik credentials terus pilih create API KEY
5. API key itu yang nantinya dipake buat dapet informasi dari youtube api

API KEY = AIzaSyCClMk9j6x5emFbtZWvaVXkLtGLSema2Ik
```

##Install Library
"""

#CRAWLINGNYA SEKARANG PAKE YUTUP API >> BABAI SELENIUM SAYANGKU CINTAKUU GALAU BRUTAL T.T

#library yang harus diinstall buat pake yutup api :)
#!pip install googleapiclient >> harus didonlot dulu
#!pip install validator >> buat nanti checking url
from googleapiclient.discovery import build #library untuk build api client
#atur konfigurasi buat build yutup api :)
API_KEY = 'AIzaSyCClMk9j6x5emFbtZWvaVXkLtGLSema2Ik' #unik untuk setiap akun
DEVELOPER_KEY = API_KEY
YOUTUBE_API_SERVICE_NAME = "youtube" #karena kita menggunakan youtube API
YOUTUBE_API_VERSION = "v3" #sesuai versi api yang kita gunakan
#buat yutup api clientnya disini
youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)

#urus keyword dari user disini
keyword = sys.argv[1]
keyword = keyword.replace("#"," ") #dari php kirimnya pake # buat batas antar kata, buat pake yutup api tinggal di spasi aja

#pake .list karena hasil outputnya banyak (berupa list)
search_response = youtube.search().list(
    q=keyword, #query search sesuai keyword kita
    part="snippet", #kita mau ambil data berupa snippet (title, desc, thumbnails, dll)
    #buat jumlah video yang mau diambil brp, bs diubah-ubah
    #buat hemat credit max hasilnya 3 video aja
    maxResults=3
).execute()

crawling_result = []  # untuk menampung hasil crawling (desc vid + komen)
# ini buat loop hasil dari searching data yang sesuai dari api
for item in search_response.get("items", []):
    if item["id"]["kind"] == "youtube#video":
        video_id = item["id"]["videoId"]  # ambil data video idnya brp (unik)

        # ambil data detail dari video
        detail_video = youtube.videos().list(
            part='snippet',  # part yang diambil adalah snippet
            id=video_id  # cari detail untuk video dgn id tertentu (sesuai yg sedang di loop)
        ).execute()

        video_info = detail_video["items"][0]["snippet"]  # ambil data snippet dari detail video
        desc_vid = video_info["description"]  # ambil deskripsi dari video
        if desc_vid != "":  # klo description videonya gak null maka masukin ke result
            crawling_result.append(desc_vid)  # masukin description ke list video

        # proses komentar
        request = youtube.commentThreads().list(
            part='snippet',  # snippert yang mau diambil
            videoId=video_id,
            textFormat='plainText',  # supaya tipe data yang dihasilkan itu berupa plaintext, jd gak ada structure html
            maxResults=3,  # ambil maksimal 3 komentar per video
            order='relevance'  # urutin berdasarkan komentar dgn top comments
        )

        comment_count = 0  # counter untuk menghitung jumlah komentar yang berhasil ditambahkan
        while request and comment_count < 3: #stop cari komentar lagi klo udah dapet 3 okayyy!!
            response = request.execute()

            # ambil data komentar dari response, lakukan looping
            for item in response['items']:
                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']  # pake text display krn gak ada tag htmlnya
                crawling_result.append(comment)
                comment_count += 1
                # klo udah dapet 3 komentar, keluar dari loop
                if comment_count >= 3:
                    break
            # lanjutkan ke section page berikutnya jika emang masih ada >> lakukan request lagi
            if comment_count < 3:  # jalanin code ini cmn klo belom dpt 3 komentar
                request = youtube.commentThreads().list_next(request, response)
#LIBRARY PREPROCESSING
import re
import validators #ini buat checking url atau bukan
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
def preprocess(text):
    split_text = text.split()  #split tiap kalimat jadi word
    #buat pola hashtag yang diterima
    #priroitas pertama cari yang dua kata atau lebih dipisahakan dengan underscore
    patternUnderscore = r"(#.*_.*)"  #klo dipisahkan oleh underscore
    patternLowUpCase = r"(#[A-Z].*[a-z][A-Z])" #klo ada lebih dari 2 kata
    splitterLowUpCase = r'(?<=[a-z])(?=[A-Z])'#ini buat splitting klo misal NasiBabixxxx gitu
    #dia mengenali yang lowercase sama uppercase sebelahan
    preprocessed_text = [] #list utk menampung hasil preprocessing :)
    for word in split_text:
        # 1. pemrosesan hashtag
        if word.startswith("#"):
            if re.search(patternUnderscore, word):  #klo equals true, maka split
                #hapus tanda #
                word = word.replace("#", "")
                res = word.split("_") #split berdasarkan tanda _
                word = " ".join(res) #hasil joinnya per item dipisahkan dengan tanda spasi
            elif re.search(patternLowUpCase, word):  #klo ditemukan yang tipenya gitu
                word = word.replace("#", "")
                res = re.split(splitterLowUpCase, word) #split tiap huruf low sama upper sebelahan
                word = " ".join(res) #hasil joinnya per item dipisahkan dengan tanda spasi
            else: #ini klo dia semisal dia ada # cmn gak memenuhi duua pola diatas, maka cuman hapus hashtag aja
                word = word.replace("#", "")
        preprocessed_text.append(word) #append ke list hasil preprocessing
    #2. case folding >> karena preprocessed_text berupa list, maka looping dan jadikan lower >> update nilai dari list
    preprocessed_text = [word.lower() for word in preprocessed_text]
    #3. penghapusan mention >> lakukan update isi lagi dgn hapus mention yang ada
    #klo semisal diloop, wordnya itu diawali dengan @, maka hapus tanda @nya
    #intinya, klo misal starts with @ nnti dia replace, klo gak ttp
    preprocessed_text = [word.replace("@", "") if word.startswith("@") else word for word in preprocessed_text]
    #4. penghapusan link dan simbol >> update lagi isi listnya
    #klo word nya kedetect validators atau url (true), maka dia gak masuk dalam list
    preprocessed_text = [word for word in preprocessed_text if not validators.url(word)]#klo return nya true (ditemukan url, maka hapus dari list)
    #5. hapus simbol yang ada
    #maksud regexnya itu, utk wordnya diarrange ulang, jd klo semisal karakternya selain huruf a-z (kapital dan lower case)
    #selain numeric, dan selain spasi, maka dia akan digantikan sama tanda " " (spasi)
    preprocessed_text = [re.sub(r'[^a-zA-Z0-9\s]', ' ', word) for word in preprocessed_text]
    #bikin string dari preprocessed_text
    text = ' '.join(preprocessed_text).strip()  #skalian hapus extraspace
    #hapus extraspace yang ada
    text = re.sub(' +', ' ', text) #klo ada 1 atau lebih spasi, maka ubah aja jadi satu spasi
    #6. stemming and stopwords removal (pakai sastrawi)
    stemmer = StemmerFactory().create_stemmer()  #create stemmer
    stopper = StopWordRemoverFactory().create_stop_word_remover() #create stopper
    stem_text = stemmer.stem(text)  #stemming
    stop_text = stopper.remove(stem_text)  #stopwords removal
    return stop_text #yang dihasilkan yang hasil udh melalui smua proses eaa

preprocessed_crawling=[] #list kosong utk simpan hasil preprocessing
for i in crawling_result: #looping tiap crawlist result, trs tiap index di preprocess
  #setelah itu disimpan di list preprocessed_crawling
  preprocessed_crawling.append(preprocess(i)) #masukin ke list hasil preprocessingnya
  
#HITUNG SIMILARITY -- COSINE DAN JACCARD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import pairwise_distances
from sklearn.metrics import jaccard_score
from sklearn.preprocessing import Binarizer
from sklearn.metrics.pairwise import cosine_similarity

#masukin ke list search querynya
preprocessed_crawling.append(preprocess(keyword)) #yang digunakan sebagai pembanding itu keyword yg udh dipreprocess
#baca method yg dipilih user
method = sys.argv[2] #argumen kedua
#TF-IDF
#buat objek tf-idf, yang nantinya digunakan untuk perhitungan similarity
tfidf=TfidfVectorizer(norm=None, sublinear_tf=True)
vector_tfidf = tfidf.fit_transform(preprocessed_crawling) #yang ditransform list yang dipake utk hitung tf-idf
distances=[] #list buat nampung distance :)
if(method=="cosine"):
     for i in range(len(preprocessed_crawling)-1):
        dist = cosine_similarity(vector_tfidf[i].toarray(), vector_tfidf[len(preprocessed_crawling)-1].toarray())
        angka = dist[0][0] #akses skalarnya krn output array 2D
        distances.append(angka)
elif(method=="jaccard"):
  binarizer=Binarizer()
  vector_tfidf_binarized=binarizer.fit_transform(vector_tfidf.toarray())
  for i in range(len(preprocessed_crawling)-1):
    #yang ini klo misal kebalik juga gapapa, karena sama aja ngehitungnya
    #query ke dokumen
    dist = jaccard_score(vector_tfidf_binarized[i],
                       vector_tfidf_binarized[len(preprocessed_crawling)-1],average='binary')
    distances.append(dist)

#masukin ke list >> buat dikirim ke user
crawling_youtube=[]
for i in range(len(crawling_result)): #looping pke angka haha
   crawling_youtube.append({
    'ori':crawling_result[i],
    'preprocessed':preprocessed_crawling[i],
    'sim':distances[i] #simpan hasil similarity
  })
#ubah output ke format json
output = json.dumps(crawling_youtube)
print(output)